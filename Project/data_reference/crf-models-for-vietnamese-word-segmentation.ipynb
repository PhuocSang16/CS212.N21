{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e0c697c",
   "metadata": {
    "papermill": {
     "duration": 0.005716,
     "end_time": "2023-03-05T02:45:51.667326",
     "exception": false,
     "start_time": "2023-03-05T02:45:51.661610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this assignment, we will build a Word Segmentation model for Vietnamese."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59580777",
   "metadata": {
    "papermill": {
     "duration": 0.004043,
     "end_time": "2023-03-05T02:45:51.675908",
     "exception": false,
     "start_time": "2023-03-05T02:45:51.671865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How to submit\n",
    "\n",
    "- Attach notebook file (.ipynb) and submit your work to Google Class Room. **Please do NOT submit URL**\n",
    "- Name your file as YourName_StudentID_Assignment3.ibynb. E.g., Nguyen_Van_A_ST099834_Assignment3.ipynb\n",
    "- Copying others' assignments is strictly prohibited.\n",
    "- Write your name and student ID into this notebook\n",
    "\n",
    "**The due for the programming assignment 3 will be at 23:59 on March 10, 2023 (Hard deadline)**\n",
    "\n",
    "- You will be deducted 5 points for each day late submission\n",
    "- Students who fail to attach the file will not be graded.\n",
    "\n",
    "# Rules\n",
    "\n",
    "- You can only use HMM or CRF model to complete the assignment.\n",
    "- If you apply HMM, it is allowed to use nltk.HiddenMarkovModelTagger to train the model.\n",
    "- Your code should run without errors\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c08b30c",
   "metadata": {
    "papermill": {
     "duration": 0.004103,
     "end_time": "2023-03-05T02:45:51.684520",
     "exception": false,
     "start_time": "2023-03-05T02:45:51.680417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Vietnamese Word Segmentation\n",
    "\n",
    "The smallest unit in the Vietnamese language is syllable (tiếng). A word may consist of one or multiple consecutive words. In some problems (such as extracting keywords from text), it is necessary to identify the words in the text.\n",
    "\n",
    "The input of the word tokenizer is a sentence consisting of syllables, and the output is a sentence with words segmented.\n",
    "\n",
    "Example:\n",
    "\n",
    "Input: Nam là sinh viên đại học ngành kỹ thuật\n",
    "\n",
    "Output: Nam là sinh_viên đại_học ngành kỹ_thuật\n",
    "\n",
    "The underscore symbol \"_\" is used to connect syllables that belong to the same word.\n",
    "\n",
    "There are many ways to solve the word segmentation problem. In this exercise, we will use a sequence labeling model to do the task. We use the BI labeling method to label each syllable in the sentence. The tag B-W is used to mark the beginning of a word, and the tag I-W is used to mark a syllable that is inside the same word as the previous syllable.\n",
    "\n",
    "If we can label each syllable in the input sentence, we can accurately tokenize the sentence.\n",
    "\n",
    "```\n",
    "Nam/B-W là/B-W sinh/B-W viên/I-W đại/B-W học/I-W ngành/B-W kỹ/B-W thuật/I-W\n",
    "```\n",
    "\n",
    "We can determine the words in the sentence from the above output. A sequence of syllables labeled as B-W I-W ... will form a word."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d22b0bc",
   "metadata": {
    "papermill": {
     "duration": 0.003985,
     "end_time": "2023-03-05T02:45:51.692748",
     "exception": false,
     "start_time": "2023-03-05T02:45:51.688763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset\n",
    "\n",
    "You will use the train data in the file [train.txt](https://drive.google.com/file/d/1Y4AuWqbInOv1HNMiGPMhuntcA-n1jfWF/view?usp=share_link) to train your Vietnamese word segmentation model and evaluate the model image on the test data in the file [test.txt](https://drive.google.com/file/d/1Y57hlYLpxVCZUbVuwOa1IRCLgUiEAx0c/view?usp=share_link) extracted from the Word Segmentation VLSP 2013 dataset.\n",
    "\n",
    "The training data contains 20000 sentences (sentences are separated by a blank line), and the test data contains 2000 sentences.\n",
    "\n",
    "You can download the file using the wget command.\n",
    "\n",
    "I have uploaded vietnamese word segmentation datasets to Kaggle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21b714ea",
   "metadata": {
    "papermill": {
     "duration": 0.004027,
     "end_time": "2023-03-05T02:45:51.701034",
     "exception": false,
     "start_time": "2023-03-05T02:45:51.697007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install necessary packages\n",
    "\n",
    "We will use following packages:\n",
    "\n",
    "- [python-crfsuite](https://github.com/scrapinghub/python-crfsuite) is a python binding to CRFsuite.\n",
    "- seqeval for sequence tagging evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7d4efa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T02:45:51.711630Z",
     "iopub.status.busy": "2023-03-05T02:45:51.710936Z",
     "iopub.status.idle": "2023-03-05T02:46:24.240022Z",
     "shell.execute_reply": "2023-03-05T02:46:24.238424Z"
    },
    "papermill": {
     "duration": 32.537628,
     "end_time": "2023-03-05T02:46:24.242847",
     "exception": false,
     "start_time": "2023-03-05T02:45:51.705219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -q seqeval[cpu]\n",
    "%pip install -q python-crfsuite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b214a903",
   "metadata": {
    "papermill": {
     "duration": 0.004215,
     "end_time": "2023-03-05T02:46:24.251729",
     "exception": false,
     "start_time": "2023-03-05T02:46:24.247514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading data\n",
    "\n",
    "We will load data into a list of tuples (word, tag) by using the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "032ad3e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T02:46:24.262818Z",
     "iopub.status.busy": "2023-03-05T02:46:24.262355Z",
     "iopub.status.idle": "2023-03-05T02:46:25.023929Z",
     "shell.execute_reply": "2023-03-05T02:46:25.022813Z"
    },
    "papermill": {
     "duration": 0.77067,
     "end_time": "2023-03-05T02:46:25.026899",
     "exception": false,
     "start_time": "2023-03-05T02:46:24.256229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load data from a file (train.txt or test.txt)\n",
    "\n",
    "    Return:\n",
    "        tagged_sentences (list): List of sentence. Each sentence is a list of tuples (word, tag)\n",
    "    \"\"\"\n",
    "    # TODO: Write your code here\n",
    "    tagged_sentences = []\n",
    "    cur_sen = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line == '':\n",
    "                if len(cur_sen) != 0:\n",
    "                    tagged_sentences.append(cur_sen)\n",
    "                    cur_sen = []\n",
    "            else:\n",
    "                word, tag = line.split()\n",
    "                cur_sen.append((word, tag))\n",
    "    if len(cur_sen) != 0:\n",
    "        tagged_sentences.append(cur_sen)\n",
    "    return tagged_sentences\n",
    "\n",
    "train_data = load_data('/kaggle/input/vietnamese-word-segmentation-datasets/train_vietnamese_word_segmentation_2013.txt')\n",
    "test_data = load_data('/kaggle/input/vietnamese-word-segmentation-datasets/test_vietnamese_word_segmentation_2013.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9a3a3b0",
   "metadata": {
    "papermill": {
     "duration": 0.004568,
     "end_time": "2023-03-05T02:46:25.036116",
     "exception": false,
     "start_time": "2023-03-05T02:46:25.031548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part 1: Building a Word Segmentation model for Vietnamese (70 points)\n",
    "\n",
    "In this section, you will build a tagging model using HMM or CRF. \n",
    "\n",
    "*Hint*: Please refer to [HMM_POS_Tagger.ipynb](https://colab.research.google.com/drive/1lcTncvhlhx8KaJ_oBW6MR7cy470MMpD9#scrollTo=Ty-Qh9Jo23dS) or [CRF_POS_Tagger.ipynb](https://colab.research.google.com/drive/1SuxBmZudn4Tn3w-pBXDoRsuSkBA7RiVV) to understand how to build a tagging model with HMM and CRF."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "010c97d3",
   "metadata": {
    "papermill": {
     "duration": 0.004169,
     "end_time": "2023-03-05T02:46:25.044811",
     "exception": false,
     "start_time": "2023-03-05T02:46:25.040642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature\n",
    "\n",
    "In this section, we are going to implement features in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d856ace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T02:46:25.055641Z",
     "iopub.status.busy": "2023-03-05T02:46:25.055206Z",
     "iopub.status.idle": "2023-03-05T02:46:25.072727Z",
     "shell.execute_reply": "2023-03-05T02:46:25.071694Z"
    },
    "papermill": {
     "duration": 0.025854,
     "end_time": "2023-03-05T02:46:25.075077",
     "exception": false,
     "start_time": "2023-03-05T02:46:25.049223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word2features(sentence, i):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        sentence (list): list of words [w1, w2,...,w_n]\n",
    "        i (int): index of the word\n",
    "    Return:\n",
    "        features (dict): dictionary of features\n",
    "    \"\"\"\n",
    "    word = sentence[i]\n",
    "    features = {\n",
    "        'is_first': i == 0,\n",
    "        'is_last': i == len(sentence) - 1,\n",
    "        'is_first_capital': word[0].isupper(),\n",
    "        'is_all_caps': int(word.upper() == word),\n",
    "        'is_all_lower': word.lower() == word,\n",
    "        'word': word,\n",
    "        'word.lower()': word.lower(),\n",
    "        'prefix_1': word[0],\n",
    "        'prefix_2': word[:2],\n",
    "        'prefix_3': word[:3],\n",
    "        'prefix_4': word[:4],\n",
    "        'suffix_1': word[-1],\n",
    "        'suffix_2': word[-2:],\n",
    "        'suffix_3': word[-3:],\n",
    "        'suffix_4': word[-4:],\n",
    "        'has_hyphen': '-' in word,\n",
    "        'is_numeric': word.isdigit(),\n",
    "        'capitals_inside': word[1:].lower() != word[1:],\n",
    "        # word unigram, bigram, and trigram\n",
    "        'word[i-2].lower()': '' if i-2<0 else sentence[i-2].lower(),\n",
    "        'word[i-1].lower()': '' if i-1<0 else sentence[i-1].lower(),\n",
    "        'word[i+1].lower()': '' if i+1>=len(sentence) else sentence[i+1].lower(),\n",
    "        'word[i+2].lower()': '' if i+2>=len(sentence) else sentence[i+2].lower(),\n",
    "\n",
    "        'word[i-2]': '' if i-2<0 else sentence[i-2],\n",
    "        'word[i-1]': '' if i-1<0 else sentence[i-1],\n",
    "        'word[i+1]': '' if i+1>=len(sentence) else sentence[i+1],\n",
    "        'word[i+2]': '' if i+2>=len(sentence) else sentence[i+2],\n",
    "\n",
    "        'words[-2,-1]': '' if i-2 < 0 else ' '.join(sentence[i-2:i]),\n",
    "        'words[-1,0]': '' if i-1 < 0 else ' '.join(sentence[i-1:i+1]),\n",
    "        'words[0,1]': '' if i+1>=len(sentence) else ' '.join(sentence[i:i+2]),\n",
    "        'words[1,2]': '' if i+2>=len(sentence) else ' '.join(sentence[i+1:i+3]),\n",
    "        'words[-2,0]': '' if i-2<0 else ' '.join(sentence[i-2:i+1]),\n",
    "        'words[-1,1]': '' if i-1<0 or i+1>=len(sentence) else ' '.join(sentence[i-1:i+1]),\n",
    "        'words[0,2]': '' if i+2>=len(sentence) else ' '.join(sentence[i:i+3]),                                                 \n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sentence):\n",
    "    \"\"\"\n",
    "    sentence is a list of words [w1, w2,...,w_n]\n",
    "    \"\"\"\n",
    "    return [word2features(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "\n",
    "def sent2labels(sentence):\n",
    "    \"\"\"\n",
    "    sentence is a list of tuples (word, postag)\n",
    "    \"\"\"    \n",
    "    return [postag for token, postag in sentence]\n",
    "\n",
    "def untag(sentence):\n",
    "    \"\"\"\n",
    "    sentence is a list of tuples (word, postag)\n",
    "    \"\"\"\n",
    "    return [token for token, _ in sentence]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64513477",
   "metadata": {
    "papermill": {
     "duration": 0.004256,
     "end_time": "2023-03-05T02:46:25.083920",
     "exception": false,
     "start_time": "2023-03-05T02:46:25.079664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's see how the feature function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c340aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T02:46:25.094542Z",
     "iopub.status.busy": "2023-03-05T02:46:25.094158Z",
     "iopub.status.idle": "2023-03-05T02:46:25.102249Z",
     "shell.execute_reply": "2023-03-05T02:46:25.101421Z"
    },
    "papermill": {
     "duration": 0.015968,
     "end_time": "2023-03-05T02:46:25.104289",
     "exception": false,
     "start_time": "2023-03-05T02:46:25.088321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_first': True,\n",
       " 'is_last': False,\n",
       " 'is_first_capital': True,\n",
       " 'is_all_caps': 0,\n",
       " 'is_all_lower': False,\n",
       " 'word': 'Nam',\n",
       " 'word.lower()': 'nam',\n",
       " 'prefix_1': 'N',\n",
       " 'prefix_2': 'Na',\n",
       " 'prefix_3': 'Nam',\n",
       " 'prefix_4': 'Nam',\n",
       " 'suffix_1': 'm',\n",
       " 'suffix_2': 'am',\n",
       " 'suffix_3': 'Nam',\n",
       " 'suffix_4': 'Nam',\n",
       " 'has_hyphen': False,\n",
       " 'is_numeric': False,\n",
       " 'capitals_inside': False,\n",
       " 'word[i-2].lower()': '',\n",
       " 'word[i-1].lower()': '',\n",
       " 'word[i+1].lower()': 'hồn',\n",
       " 'word[i+2].lower()': 'nhiên',\n",
       " 'word[i-2]': '',\n",
       " 'word[i-1]': '',\n",
       " 'word[i+1]': 'hồn',\n",
       " 'word[i+2]': 'nhiên',\n",
       " 'words[-2,-1]': '',\n",
       " 'words[-1,0]': '',\n",
       " 'words[0,1]': 'Nam hồn',\n",
       " 'words[1,2]': 'hồn nhiên',\n",
       " 'words[-2,0]': '',\n",
       " 'words[-1,1]': '',\n",
       " 'words[0,2]': 'Nam hồn nhiên'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(untag(train_data[0]))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0371b73e",
   "metadata": {
    "papermill": {
     "duration": 0.004397,
     "end_time": "2023-03-05T02:46:25.113615",
     "exception": false,
     "start_time": "2023-03-05T02:46:25.109218",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we can extract features from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8339dd32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T02:46:25.126114Z",
     "iopub.status.busy": "2023-03-05T02:46:25.125374Z",
     "iopub.status.idle": "2023-03-05T02:46:32.093494Z",
     "shell.execute_reply": "2023-03-05T02:46:32.092497Z"
    },
    "papermill": {
     "duration": 6.976812,
     "end_time": "2023-03-05T02:46:32.096131",
     "exception": false,
     "start_time": "2023-03-05T02:46:25.119319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = [sent2features(untag(s)) for s in train_data]\n",
    "y_train = [sent2labels(s) for s in train_data]\n",
    "\n",
    "X_test = [sent2features(untag(s)) for s in test_data]\n",
    "y_test = [sent2labels(s) for s in test_data]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d87a332e",
   "metadata": {
    "papermill": {
     "duration": 0.004378,
     "end_time": "2023-03-05T02:46:32.105360",
     "exception": false,
     "start_time": "2023-03-05T02:46:32.100982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training\n",
    "To see all possible CRF parameters check its docstring. Here we are using SGD training algorithm with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa1708d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T02:46:32.116442Z",
     "iopub.status.busy": "2023-03-05T02:46:32.115852Z",
     "iopub.status.idle": "2023-03-05T02:48:53.394710Z",
     "shell.execute_reply": "2023-03-05T02:48:53.393319Z"
    },
    "papermill": {
     "duration": 141.292555,
     "end_time": "2023-03-05T02:48:53.402419",
     "exception": false,
     "start_time": "2023-03-05T02:46:32.109864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 20s, sys: 925 ms, total: 2min 21s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pycrfsuite\n",
    "\n",
    "trainer = pycrfsuite.Trainer(algorithm='lbfgs', verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "\n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "trainer.train('word_segmenter.crfsuite')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bc1e0c7",
   "metadata": {
    "papermill": {
     "duration": 0.004524,
     "end_time": "2023-03-05T02:48:53.412011",
     "exception": false,
     "start_time": "2023-03-05T02:48:53.407487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part 2: Model evaluation (30 points)\n",
    "Evaluation measures:\n",
    "- P(recision): (Number of word models correctly split)/(Number of words in the model's output)\n",
    "- R(ecall): (Number of word models correctly split)/(Number of words in ground-truth data)\n",
    "- F1 = 2PR/(P+R)\n",
    "\n",
    "We first load model to a tagger and then predict on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0adf8eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T02:48:53.423139Z",
     "iopub.status.busy": "2023-03-05T02:48:53.422696Z",
     "iopub.status.idle": "2023-03-05T02:48:59.101581Z",
     "shell.execute_reply": "2023-03-05T02:48:59.099963Z"
    },
    "papermill": {
     "duration": 5.6878,
     "end_time": "2023-03-05T02:48:59.104465",
     "exception": false,
     "start_time": "2023-03-05T02:48:53.416665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           W       0.96      0.97      0.96     62131\n",
      "\n",
      "   micro avg       0.96      0.97      0.96     62131\n",
      "   macro avg       0.96      0.97      0.96     62131\n",
      "weighted avg       0.96      0.97      0.96     62131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('word_segmenter.crfsuite')\n",
    "predicted_tag_sequences = [tagger.tag(xseq) for xseq in X_test]\n",
    "\n",
    "print(classification_report(y_test, predicted_tag_sequences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 200.071712,
   "end_time": "2023-03-05T02:49:01.831776",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-05T02:45:41.760064",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
